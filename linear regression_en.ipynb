{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**linear regression**\n",
    "\n",
    "In this notebook the linear regression is demonstrated in 5 examples:<br>\n",
    "(note: internal links are only valid of the linked cell is already run)<br>\n",
    "[example_1](#example_1): some datpoints with low noise<br>\n",
    "[example_2](#example_2): more datapoints and stronger distotion<br>\n",
    "[example_3](#example_3): Linear Regression with outliers<br>\n",
    "[example_4](#example_4): regularized regression with ridge regression and lasso<br>\n",
    "[example_5](#example_5): Regularized Regression with polynomial dataset<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='calc_regression'></a>\n",
    "### def calc_regression():\n",
    "**function to define a linear model and fit the given data to the model**<br>\n",
    "[sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>\n",
    "\n",
    "LinearRegression fits a linear model with coefficients $ \\omega = \\left\\{\\omega_1, ...,\\omega_p\\right\\}$\n",
    "to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:\n",
    "$$\\min\\limits_{\\omega}||X\\omega - y||_2^2 $$\n",
    "more information about the class LinearRegression: [class LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
    "\n",
    "in ``calc_regression()`` the linear model will be created and fit to the given datapoints. It is possible to do the fitting with all datapoints or with a part of them (training).<br> The fitted model will be used to predict values along the x-Axis of the given datapoints. The accuricy of the linear model is represented due to [score()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) and the two attributes ``coef_`` and ``intercept_``.\n",
    "calc_regression() returns a dictionary containig the linear model and its datapoints as well as all information about the linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regression(data_points, training=False, test_size=None):\n",
    "    #\n",
    "    # create linear regression model\n",
    "    reg_model = linear_model.LinearRegression()\n",
    "    #\n",
    "    # split datapoints matrix in x and y values\n",
    "    #\n",
    "    # --> np.array.reshape(-1,1): -1: unknown size of row\n",
    "    #                              1: 1 column\n",
    "    #                    result:  a vertical vector of dataPoints with nRows(=len(dataPoints)) and 1 column \n",
    "    #\n",
    "    X = data_points[:,0].reshape(-1,1)\n",
    "    Y = data_points[:,1].reshape(-1,1)\n",
    "    #\n",
    "    # calc residual sum of squares\n",
    "    # there are in general 2 ways of using the linear regression model\n",
    "    # - calc residual sum of squares with all given datapoints\n",
    "    # - devide the given datapoints into training and test data. \n",
    "    if training:\n",
    "        # split data into training and test data\n",
    "        #\n",
    "        # here x% of the dataPoints are used to train the algortihm\n",
    "        # --> parameter: 'test_size'\n",
    "        #\n",
    "        # parameter random_state set to a fix value in order to get always the same result with the same data\n",
    "        #\n",
    "        if test_size == None:\n",
    "            test_size = 0.2\n",
    "        #\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=5)\n",
    "        #\n",
    "        # create LinearRegression model and train it with training data\n",
    "        #\n",
    "        reg_model = linear_model.LinearRegression()\n",
    "        reg_model.fit(X_train, Y_train)\n",
    "    else:\n",
    "        reg_model.fit(X,Y)\n",
    "        #\n",
    "        #\n",
    "    # test regModel when predicting along the x-axis:\n",
    "    predicted_values = reg_model.predict(X)\n",
    "    #\n",
    "    dict_lin_regression = {\n",
    "        'model': reg_model,\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'y_predict': predicted_values,\n",
    "        'coef': reg_model.coef_,\n",
    "        'intercept': reg_model.intercept_,\n",
    "        'score': reg_model.score(X,Y)}\n",
    "    \n",
    "    return(dict_lin_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='get_Data_Points'></a>\n",
    "### def get_Data_Points():\n",
    "**function to define a 2 dimensional array with points on a linear function**<br>\n",
    "\n",
    "input parameters:\n",
    "- n_points\n",
    "- slope\n",
    "- intercept\n",
    "- xmax\n",
    "- xmin (default:0)\n",
    "\n",
    "$\\text{get_Data_Points()}$ returns a 2dimensional numpy-array with points of the linear function: y = slope * x + intercept.\n",
    "- column_0: linear spaced x-values from xmin...xmax  \n",
    "- column_1: y-values according to the linear function with the parameter slope and intercept\n",
    "(column size depends on nPoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Data_Points(n_points, slope, intercept, xmax, xmin=0):\n",
    "    #\n",
    "    data_points = np.empty([n_points, 2])\n",
    "    data_points[:,0] = np.linspace(xmin, xmax, n_points)\n",
    "    data_points[:,1] = data_points[:,0]*slope + intercept\n",
    "    #\n",
    "    return(data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_noiseVec'></a>\n",
    "### def create_noise_vec():\n",
    "**function to define a vector containig noise**<br>\n",
    "\n",
    "input parameter:\n",
    "- distortion\n",
    "- n_length\n",
    "- noise_type (default: 'gaussian')\n",
    "- n_columns  (default: 2)\n",
    "\n",
    "random generator:\n",
    "numpy.random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise_vec(distortion, n_length, noise_type='gaussian', n_columns=2):\n",
    "    #\n",
    "    # create a vector of given size with noise\n",
    "    #\n",
    "    if noise_type == 'gaussian':\n",
    "        mu = 0 # parameter mu has no effect on the distortion\n",
    "        noise_vec = np.random.normal(mu, distortion, [n_length, n_columns]) \n",
    "    #\n",
    "    return(noise_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_1'></a><br>**Example 1**\n",
    "\n",
    "simple dataset with a few points and not much noise.\n",
    "\n",
    "Create dataset using function [get_Data_Points()](#get_DataPoints) und [create_noise_vec()](#create_noiseVec). The predefined parameters allow following linear function:<br>\n",
    "- $y = 0.45 \\bullet x + 0.5$\n",
    "- total number of points:    20\n",
    "- x-range: 0...10\n",
    "<br>\n",
    "- distortion: 0.4\n",
    "\n",
    "After regression is computed with [calc_regression](#calc_regression) the predicted values are plotted together with the noisy datapoints and the 'true' linear function.\n",
    "\n",
    "more info about linear regression: [sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split \n",
    "#\n",
    "# generate a simple dataset\n",
    "#\n",
    "# create 20 datapoints\n",
    "#\n",
    "x_min = 0\n",
    "x_max = 10\n",
    "#\n",
    "n_points = 20 \n",
    "#\n",
    "slope = 0.45 \n",
    "#\n",
    "intercept = 0.5\n",
    "#\n",
    "clear_linear_data = get_Data_Points(n_points, slope, intercept, x_max, x_min)\n",
    "#\n",
    "# create some noise\n",
    "#\n",
    "noise_data = create_noise_vec(0.4, n_points)\n",
    "#\n",
    "# add noise to the linear points\n",
    "data = clear_linear_data + noise_data\n",
    "#\n",
    "# calculate linear regression\n",
    "#\n",
    "dict_reg = calc_regression(data)\n",
    "#\n",
    "#---------------------------------------------------------------------\n",
    "#\n",
    "# plot data\n",
    "#\n",
    "# \n",
    "x_points = dict_reg[\"X\"]\n",
    "y_points = dict_reg[\"Y\"]\n",
    "#\n",
    "y_predict = dict_reg[\"y_predict\"]\n",
    "#\n",
    "# all points (with little noise)\n",
    "plt.scatter(x_points, y_points, color='gray', label=\"datapoints (noisy)\")\n",
    "# regression line from regression model\n",
    "plt.plot(x_points, y_predict, color='red', linewidth=2, label=\"regression line\")\n",
    "# linear function of clean data (y = m*x + b)\n",
    "plt.plot(clear_linear_data[:,0], clear_linear_data[:,1], color='orange', lw=2, label=\"linear function: y={}*x+{}\".format(slope, intercept))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_2'></a><br>**Example 2**\n",
    "\n",
    "more datapoints and stronger distotion:\n",
    "\n",
    "Create dataset using function [get_Data_Points()](#get_DataPoints) und [create_noise_Vec()](#create_noiseVec). The predefined parameters allow following linear function:<br>\n",
    "- y = 0.45 * x + 0.5 <br>\n",
    "- total number of points: 200 <br>\n",
    "- x-range: 0...10 <br>\n",
    "- distortion: 4.0\n",
    "\n",
    "After regression is computed with [calc_regression](#calc_regression) the predicted values are plotted together with the noisy datapoints and the 'true' linear function.<br>\n",
    "more info about linear regression: [sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate a simple dataset with strong distortion\n",
    "#\n",
    "#\n",
    "x_min = 0\n",
    "x_max = 10\n",
    "#\n",
    "n_points = 200 # total number of points \n",
    "#\n",
    "slope = 0.45 #(m)\n",
    "#\n",
    "intercept = 0.5 #(b)\n",
    "#\n",
    "clear_lineardata = get_Data_Points(n_points, slope, intercept, x_max, x_min)\n",
    "#\n",
    "# create some noise\n",
    "#\n",
    "noise_vec = create_noise_vec(4.0, n_points)\n",
    "#\n",
    "# add noise to the linear points\n",
    "noisy_data = clear_lineardata + noise_vec\n",
    "#\n",
    "# calc linear regression\n",
    "#\n",
    "dict_reg = calc_regression(noisy_data)\n",
    "#\n",
    "#\n",
    "# plot data\n",
    "# \n",
    "x_points = dict_reg[\"X\"]\n",
    "y_points = dict_reg[\"Y\"]\n",
    "#\n",
    "y_predicted = dict_reg[\"y_predict\"]\n",
    "#\n",
    "# all points (with little noise)\n",
    "plt.scatter(x_points, y_points, color='gray', label=\"datapoints (noisy)\")\n",
    "# regression line from regression model\n",
    "plt.plot(x_points, y_predicted, color='red', linewidth=2, label=\"regression line\")\n",
    "# linear function of clean data (y = m*x + b)\n",
    "plt.plot(clear_lineardata[:,0], clear_lineardata[:,1], color='orange', lw=2, label=\"linear function: y={}*x+{}\".format(slope, intercept))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_3'></a><br>**Example 3**\n",
    "\n",
    "#### Linear Regression with outliers\n",
    "\n",
    "Create dataset using function [get_Data_Points()](#get_DataPoints) und [create_noise_Vec()](#create_noiseVec). The predefined parameters allow following linear function:<br>\n",
    "- y = $0.45\\cdot x + 0.5 $\n",
    "- nPoints:  100 (total number of points)\n",
    "- xMin, xMax: 0...5 (x-range)\n",
    "- nOutliers: 5 (number of outlier points) \n",
    "<br>\n",
    "- distortion: 0.6\n",
    "\n",
    "After regression is computed with [calc_regression](#calc_regression) the predicted values are plotted together with the noisy datapoints and the 'true' linear function.<br>\n",
    "more info about linear regression: [sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate a simple dataset with some noise and outlier points\n",
    "#\n",
    "np.random.seed(0)\n",
    "#\n",
    "x_min = 0\n",
    "x_max = 5\n",
    "#\n",
    "n_points = 1000  \n",
    "#\n",
    "slope = 0.45\n",
    "#\n",
    "intercept = 0.5\n",
    "#\n",
    "n_outliers = 5\n",
    "#\n",
    "distortion = 0.6\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "# define clear dataset\n",
    "clear_lineardata = get_Data_Points((n_points + n_outliers), slope, intercept, x_max, x_min)\n",
    "#\n",
    "# add Outliers\n",
    "clear_lineardata[n_points:,0] = 15 + 0.5 * np.random.normal(size=(n_outliers))\n",
    "clear_lineardata[n_points:,1] = -3 - 10 * np.random.normal(size=n_outliers)\n",
    "#\n",
    "# create some noise\n",
    "noise_vec = create_noise_vec(distortion, (n_points + n_outliers))\n",
    "# add noise to the linear points\n",
    "noisydata = clear_lineardata + noise_vec\n",
    "#\n",
    "# calc linear regression\n",
    "#\n",
    "dict_reg = calc_regression(noisydata)\n",
    "#\n",
    "######################################\n",
    "# plot data\n",
    "#\n",
    "# \n",
    "x_points = dict_reg[\"X\"]\n",
    "y_points = dict_reg[\"Y\"]\n",
    "#\n",
    "y_predict = dict_reg[\"y_predict\"]\n",
    "#\n",
    "# all points (with little noise)\n",
    "plt.scatter(x_points, y_points, color='gray', label=\"datapoints (noisy)\")\n",
    "# regression line from regression model\n",
    "plt.plot(x_points, y_predict, color='red', linewidth=2, label=\"regression line\")\n",
    "# linear function of clean data (y = m*x + b)\n",
    "plt.plot(clear_lineardata[0:n_points,0], clear_lineardata[0:n_points,1], color='orange', lw=2, label=\"linear function: y={}*x+{}\".format(slope, intercept))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularized_regression'>**Regularized Regression**</a>\n",
    "\n",
    "In the following examples 2 methods are shown:\n",
    "- ridge regression\n",
    "- Lasso\n",
    "\n",
    "###### Ridge regression\n",
    "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:\n",
    "$$ \\min\\limits_{\\omega} || X\\omega-y||_2^2 + \\alpha ||\\omega||_2^2$$\n",
    "more info about ridge regression: [scikit-learn.ridge-regession:](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)<br>\n",
    "\n",
    "###### Lasso\n",
    "The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing.\n",
    "Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:\n",
    "$$ \\min\\limits_{\\omega} \\frac{1}{2n_{samples}} || X\\omega-y||_2^2 + \\alpha ||\\omega||_1$$\n",
    " \n",
    "more info about Lasso: [scikit-learn.ridge-regession:](https://scikit-learn.org/stable/modules/linear_model.html#lasso)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example_4\"></a><br>\n",
    "**example 4**\n",
    "regularized regression with ridge regression and lasso\n",
    "\n",
    "In this example the slope and intercept value from the ridge and lasso regression will be displayd in a histogram plot. To demonstrate the 2 methods, 3 datasets with different slopes will be created and fitted to the both models. This process will be repeated for 1000 times.\n",
    "\n",
    "functions:\n",
    "- f1: $y=0.0\\cdot x+ 1.0$\n",
    "- f2: $y=0.2\\cdot x+ 1.0$\n",
    "- f3: $y=2.0\\cdot x+ 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "alpha = 1\n",
    "#\n",
    "x_min = 0\n",
    "#\n",
    "x_max = 5\n",
    "#\n",
    "n_points = 500\n",
    "#\n",
    "distortion= 0.5\n",
    "#\n",
    "n_repeats = 1000\n",
    "#\n",
    "slopes=[0.0, 0.2, 2.0]\n",
    "#\n",
    "intercept = [1.0, 1.0, 1.0]\n",
    "#\n",
    "coef_result = np.empty([n_repeats,6])\n",
    "intercpt_result = np.empty([n_repeats,6])\n",
    "#\n",
    "ridgemodel = Ridge(alpha=alpha)\n",
    "lassomodel = Lasso(alpha=alpha, max_iter=1e5)\n",
    "#\n",
    "for i in range(n_repeats):\n",
    "    #\n",
    "    noise = create_noise_vec(distortion, n_points)\n",
    "    #\n",
    "    set1 = get_Data_Points(n_points, slopes[0], intercept[0], x_max, x_min) + noise\n",
    "    set2 = get_Data_Points(n_points, slopes[1], intercept[1], x_max, x_min) + noise\n",
    "    set3 = get_Data_Points(n_points, slopes[2], intercept[2], x_max, x_min) + noise\n",
    "    #\n",
    "    # fit and analyse set1\n",
    "    ridgemodel.fit(set1[:,0].reshape(-1,1), set1[:,1].reshape(-1,1))\n",
    "    coef_result[i,0] = ridgemodel.coef_\n",
    "    intercpt_result[i,0] = ridgemodel.intercept_\n",
    "    #\n",
    "    lassomodel.fit(set1[:,0].reshape(-1,1), set1[:,1].reshape(-1,1))\n",
    "    coef_result[i,3] = lassomodel.coef_\n",
    "    intercpt_result[i,3] = lassomodel.intercept_\n",
    "    #\n",
    "    # fit and analyse set2\n",
    "    ridgemodel.fit(set2[:,0].reshape(-1,1), set2[:,1].reshape(-1,1))\n",
    "    coef_result[i,1] = ridgemodel.coef_\n",
    "    intercpt_result[i,1] = ridgemodel.intercept_\n",
    "    #\n",
    "    lassomodel.fit(set2[:,0].reshape(-1,1), set2[:,1].reshape(-1,1))\n",
    "    coef_result[i,4] = lassomodel.coef_\n",
    "    intercpt_result[i,4] = lassomodel.intercept_\n",
    "    #\n",
    "    # fit and analyse set3\n",
    "    ridgemodel.fit(set3[:,0].reshape(-1,1), set3[:,1].reshape(-1,1))\n",
    "    coef_result[i,2] = ridgemodel.coef_\n",
    "    intercpt_result[i,2] = ridgemodel.intercept_\n",
    "    #\n",
    "    lassomodel.fit(set3[:,0].reshape(-1,1), set3[:,1].reshape(-1,1))\n",
    "    coef_result[i,5] = lassomodel.coef_\n",
    "    intercpt_result[i,5] = lassomodel.intercept_\n",
    "    #\n",
    "#----------------------------------------------------------------\n",
    "#\n",
    "# plot results\n",
    "#\n",
    "# slopes:                        interceptions:\n",
    "# |--0..2--| ... |--3...5--|     |--0..2--| ... |--3...5--|\n",
    "# |ridge() | ... | lasso()|      |ridge() | ... | lasso()|\n",
    "# | set1-3 | ... | set1-3 |      | set1-3 | ... | set1-3 |\n",
    "#\n",
    "#\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15,4), sharey=True)\n",
    "axes[0].hist(coef_result[:,0:3], 55)\n",
    "axes[1].hist(intercpt_result[:,0:3], 13)\n",
    "axes[2].hist(coef_result[:,4:6], 55)\n",
    "axes[3].hist(intercpt_result[:,4:6], 13)\n",
    "#\n",
    "axes[0].set_xticks(slopes)\n",
    "# axes[0].set_ylim([0,1000])\n",
    "axes[1].set_xticks(intercept)\n",
    "axes[2].set_xticks(slopes)\n",
    "axes[3].set_xticks(intercept)\n",
    "\n",
    "#\n",
    "str0 = r\"ridge-regression with $\\alpha = {{{}}}$\".format(alpha)\n",
    "str0 += \"\\n\"\n",
    "str0 += r\"y = {} $\\bullet$ x + {}\".format(slopes[0], intercept[0])\n",
    "str0 += \"\\n\"\n",
    "str0 += r\"y = {} $\\bullet$ x + {}\".format(slopes[1], intercept[1])\n",
    "str0 += \"\\n\"\n",
    "str0 += r\"y = {} $\\bullet$ x + {}\".format(slopes[2], intercept[2])\n",
    "axes[0].set_title(str0)\n",
    "axes[0].legend(['slope: %.1f'%slopes[0], 'slope: %.1f'%slopes[1], 'slope: %.1f'%slopes[2]])\n",
    "axes[1].set_title(str0)\n",
    "axes[1].legend(['intercept: %.1f' %intercept[0], 'intercept: %.1f' %intercept[1], 'intercept: %.1f' %intercept[2]])\n",
    "#\n",
    "str1 = str0.replace('ridge', 'lasso')\n",
    "axes[2].set_title(str1)\n",
    "axes[2].legend(['slope: %.1f'%slopes[0], 'slope: %.1f'%slopes[1], 'slope: %.1f'%slopes[2]])\n",
    "axes[3].set_title(str1)\n",
    "axes[3].legend(['intercept: %.1f' %intercept[0], 'intercept: %.1f' %intercept[1], 'intercept: %.1f' %intercept[2]])\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example_5\"></a>**Example 5**\n",
    "\n",
    "Regularized Regression with polynomial dataset\n",
    "\n",
    "A polynomial function will be represented with just a litte amount of data points. This training data will be used for different regression models. The result can be seen in the following diagrams:\n",
    "\n",
    "polynomial function:\n",
    "$y= -10 \\cdot x^3 - 2.3 \\cdot x^2 + 0.1\\cdot x -2$\n",
    "\n",
    "number of training points (nPoints): 40<br>\n",
    "x_min: -2<br>\n",
    "x_max: 2<br>\n",
    "distortion: 0.5<br>\n",
    "alpha ridge: 0.5<br>\n",
    "alpha lasso: 0.5<br>\n",
    "degree polyfit: 3<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso \n",
    "#\n",
    "# sample function: y = -10x³ - 2.3x² + 0.1x -2\n",
    "#\n",
    "def f(x):\n",
    "    return(-10*x**3 - 2.3*x**2 +0.1*x -2)\n",
    "\n",
    "#\n",
    "# create dataset\n",
    "# generate points and keep a subset of them\n",
    "x_min=-2\n",
    "x_max=2\n",
    "#\n",
    "n_points = 20\n",
    "#\n",
    "alpha_ridge = 2.0\n",
    "alpha_lasso = 3.0\n",
    "degree_polyfit = 3\n",
    "#\n",
    "#\n",
    "x_0 = np.linspace(x_min,x_max,200)\n",
    "x_plot = np.linspace(x_min,x_max,200)\n",
    "#\n",
    "#\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x_0)\n",
    "x = np.sort(x_0[:n_points])\n",
    "y = f(x)\n",
    "#\n",
    "X = x[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "#--------------------------------------------------\n",
    "#\n",
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "# linear regression\n",
    "#\n",
    "polynomial_features = PolynomialFeatures(degree=degree_polyfit)\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", LinearRegression())])\n",
    "# linModel = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
    "pipeline.fit(X,y)\n",
    "LinPlot = pipeline.predict(X_plot)\n",
    "plt.plot(x_plot, LinPlot, color=colors[0], linewidth=lw, label=\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "# poly fitting\n",
    "polyFit = np.poly1d(np.polyfit(x,y,3))\n",
    "plt.plot(x_plot, polyFit(x_plot), color=colors[1], lw=lw, label=\"polyfit\")\n",
    "#\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "#LR + ridge\n",
    "LR_ridge = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=degree_polyfit)),\n",
    "                         (\"linear_regression\", Ridge(alpha=alpha_ridge))\n",
    "                    ])\n",
    "LR_ridge.fit(X,y)\n",
    "yLR_ridge = LR_ridge.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_ridge, color=colors[2],lw=lw,label=\"LR + ridge\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "\n",
    "#LR + lasso\n",
    "LR_lasso = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=degree_polyfit)),\n",
    "                         (\"linear_regression\", Lasso(alpha = alpha_lasso))\n",
    "                    ])\n",
    "LR_lasso.fit(X,y)\n",
    "yLR_lasso = LR_lasso.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_lasso, color=colors[3],lw=lw,label=\"LR + Lasso\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot all regression results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "# linear regression\n",
    "#\n",
    "polynomial_features = PolynomialFeatures(degree=degree_polyfit)\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", LinearRegression())])\n",
    "#\n",
    "pipeline.fit(X,y)\n",
    "LinPlot = pipeline.predict(X_plot)\n",
    "plt.plot(x_plot, LinPlot, color=colors[0], linewidth=lw, label=\"Linear Regression\")\n",
    "#\n",
    "# poly fitting\n",
    "polyFit = np.poly1d(np.polyfit(x,y,3))\n",
    "plt.plot(x_plot, polyFit(x_plot), color=colors[1], lw=lw, label=\"polyfit\")\n",
    "#\n",
    "#LR + ridge\n",
    "LR_ridge = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=4)),\n",
    "                         (\"linear_regression\", Ridge(alpha=alpha_ridge))\n",
    "                    ])\n",
    "LR_ridge.fit(X,y)\n",
    "yLR_ridge = LR_ridge.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_ridge, color=colors[2],lw=lw,label=\"LR + ridge\")\n",
    "#\n",
    "#LR + lasso\n",
    "LR_lasso = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=6)),\n",
    "                         (\"linear_regression\", Lasso(alpha=alpha_lasso))])\n",
    "LR_lasso.fit(X,y)\n",
    "yLR_lasso = LR_lasso.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_lasso, color=colors[3],lw=lw,label=\"LR + Lasso\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 IUBH Internationale Hochschule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
