{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 IUBH Internationale Hochschule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**linear regression**\n",
    "\n",
    "\"In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. [2]\"\n",
    "\n",
    "[3]\n",
    "\n",
    "In the term of machine learning, linear regression is a supervised learning algorithm.\n",
    "With scikit-leran a simple linear regression model uses the ordinary least square method to identify the linear parameters fitting the given dataset.\n",
    "\n",
    "In this notebook the linear regression is demonstrated in 5 examples:<br>\n",
    "(note: internal links are only valid of the linked cell is already run)<br>\n",
    "[example_1](#example_1): some datpoints with low noise<br>\n",
    "[example_2](#example_2): more datapoints and stronger distotion<br>\n",
    "[example_3](#example_3): Linear Regression with outliers<br>\n",
    "[example_4](#example_4): regularized regression with ridge regression and lasso<br>\n",
    "[example_5](#example_5): Regularized Regression with polynomial dataset<br>\n",
    "\n",
    "\n",
    "[1](https://en.wikipedia.org/wiki/Linear_regression#cite_note-Freedman09-1)\n",
    "[2](https://en.wikipedia.org/wiki/Linear_regression#cite_note-2)\n",
    "[3](https://en.wikipedia.org/wiki/Linear_regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='calc_regression'></a>\n",
    "### def calc_regression():\n",
    "**function to define a linear model and fit the given data to the model**<br>\n",
    "[sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>\n",
    "\n",
    "\"LinearRegression fits a linear model with coefficients $ \\omega = \\left\\{\\omega_1, ...,\\omega_p\\right\\}$\n",
    " to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:\n",
    "$$\\min\\limits_{\\omega}||X\\omega - y||_2^2 $$\n",
    "LinearRegression will take in its fit method arrays $X, y$ and will store the coefficients $\\omega$ of the linear model in its $\\text{coef_}$ member. The coefficient estimates for Ordinary Least Squares rely on the independence of the features.\n",
    "When features are correlated and the columns of the design matrix $X$ have an approximate linear dependence, the design \n",
    "matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors\n",
    "in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, \n",
    "when data are collected without an experimental design.\n",
    "<br>\n",
    "[class LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression):<br>\n",
    "methods (most important):\n",
    "- fit(X, y[, sample_weight])     : fit linear model either with all data or with a part of the data\n",
    "- predict(X) : Predict using the linear model.\n",
    "- score(X, y[, smaple_weigth])   : Return the coefficient of determination R^2 of the prediction.\n",
    "    \n",
    "attributes<br>\n",
    "- $\\text{coef_}$ : array of shape (n_features, ) or (n_targets, n_features)<br> Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features),while if only one target is passed, this is a 1D array of length n_features.\n",
    "- $\\text{intercept_}$: float or array of shape (n_targets,)<br> Independent term in the linear model. Set to 0.0 if $\\text{fit_intercept} = False$.\n",
    "\n",
    "in $\\text{calc_regression()}$ the linear model will be created and fit to the given datapoints. It is possible to do the fitting with all datapoints or with a part of them (training).<br> The fitted model will be used to predict values along the x-Axis of the given datapoints. The accuricy of the linear model is represented due to score() and the two attributes $\\text{coef_}$ and $\\text{intercept_}$.\n",
    "calc_regression returns a dictionary containig the linear model and its datapoints as well as all information about the linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regression(dataPoints, training=False, testSize=None):\n",
    "    #\n",
    "    # create linear regression model\n",
    "    regModel = linear_model.LinearRegression()\n",
    "    #\n",
    "    # split datapoints matrix in x and y values\n",
    "    #\n",
    "    # --> np.array.reshape(-1,1): -1: unknown size of row\n",
    "    #                              1: 1 column\n",
    "    #                    result:  a vertical vector of dataPoints with nRows(=len(dataPoints)) and 1 column \n",
    "    #\n",
    "    X = dataPoints[:,0].reshape(-1,1)\n",
    "    Y = dataPoints[:,1].reshape(-1,1)\n",
    "    #\n",
    "    # calc residual sum of squares\n",
    "    # there are in general 2 ways of using the linear regression model\n",
    "    # - calc residual sum of squares with all given datapoints\n",
    "    # - devide the given datapoints into training and test data. \n",
    "    if training:\n",
    "        # split data into training and test data\n",
    "        #\n",
    "        # here 80% of the dataPoints are used to train the algortihm\n",
    "        # --> parameter: 'test_size'\n",
    "        #\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=testSize, random_state=0)\n",
    "        #\n",
    "        # create LinearRegression model and train it with training data\n",
    "        #\n",
    "        regModel = linear_model.LinearRegression()\n",
    "        regModel.fit(X_train, Y_train)\n",
    "    else:\n",
    "        regModel.fit(X,Y)\n",
    "        #\n",
    "        #\n",
    "    # test regModel when predicting along the x-axis:\n",
    "    predictValues = regModel.predict(X)\n",
    "    #\n",
    "    dictLinRegression = {\n",
    "        'model': regModel,\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'yPredict': predictValues,\n",
    "        'coef': regModel.coef_,\n",
    "        'intercept': regModel.intercept_,\n",
    "        'score': regModel.score(X,Y)}\n",
    "    \n",
    "    return(dictLinRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='get_DataPoints'></a>\n",
    "### def get_DataPoints():\n",
    "**function to define a 2 dimensional array with points on a linear function**<br>\n",
    "\n",
    "input parameters:\n",
    "- nPoints\n",
    "- slope\n",
    "- intercept\n",
    "- xmax\n",
    "- xmin (default:0)\n",
    "\n",
    "$\\text{get_DataPoints()}$ returns a 2dimensional numpy-array with points of the linear function: y = slope * x + intercept.\n",
    "- column_0: linear spaced x-values from xmin...xmax  \n",
    "- column_1: y-values according to the linear function with the parameter slope and intercept\n",
    "(column size depends on nPoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DataPoints(nPoints, slope, intercept,xmax, xmin=0):\n",
    "    #\n",
    "    dataPoints = np.empty([nPoints,2])\n",
    "    dataPoints[:,0] = np.linspace(xmin, xMax, nPoints)\n",
    "    dataPoints[:,1] = dataPoints[:,0]*slope + intercept\n",
    "    #\n",
    "    return(dataPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_noiseVec'></a>\n",
    "### def create_noiseVec():\n",
    "**function to define a vector containig noise**<br>\n",
    "\n",
    "input parameter:\n",
    "- distortion\n",
    "- nLength\n",
    "- noiseType (default: 'gaussian')\n",
    "- nColumns  (default: 2)\n",
    "\n",
    "random generator:\n",
    "numpy.random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noiseVec(distortion, nLength, noiseType='gaussian', nColumns=2):\n",
    "    #\n",
    "    # create a vector of given size with noise\n",
    "    #\n",
    "    if noiseType == 'gaussian':\n",
    "        mu = 0 # parameter mu has no effect on the distortion\n",
    "        noiseVec = np.random.normal(mu, distortion, [nLength, nColumns]) \n",
    "    #\n",
    "    return(noiseVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_1'></a><br>**Example 1**\n",
    "\n",
    "simple dataset with a few points and not much noise.\n",
    "\n",
    "Create dataset using function [get_DataPoints()](#get_DataPoints) und [create_noiseVec()](#create_noiseVec). The predefined parameters allow following linear function:<br>\n",
    "- $y = 0.45 \\bullet x + 0.5$\n",
    "- total number of points:    20\n",
    "- x-range: 0...10\n",
    "<br>\n",
    "- distortion: 0.4\n",
    "<br>\n",
    "After regression is computed with [calc_regression](#calc_regression) the predicted values are plotted together with the noisy datapoints and the 'true' linear function.\n",
    "\n",
    "more info about linear regression: [sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split \n",
    "#\n",
    "# generate a simple dataset\n",
    "#\n",
    "# create 20 datapoints\n",
    "#\n",
    "xMin = 0\n",
    "xMax = 10\n",
    "#\n",
    "nPoints = 20 \n",
    "#\n",
    "slope = 0.45 \n",
    "#\n",
    "intercept = 0.5\n",
    "#\n",
    "clearLinData = get_DataPoints(nPoints, slope, intercept, xMax, xMin)\n",
    "#\n",
    "# create some noise\n",
    "#\n",
    "noiseData = create_noiseVec(0.4, nPoints)\n",
    "#\n",
    "# add noise to the linear points\n",
    "noisyPoints = clearLinData + noiseData\n",
    "#\n",
    "# calculate linear regression\n",
    "#\n",
    "dictReg = calc_regression(noisyPoints)\n",
    "#\n",
    "#---------------------------------------------------------------------\n",
    "#\n",
    "# plot data\n",
    "#\n",
    "# \n",
    "xPoints = dictReg[\"X\"]\n",
    "yPoints = dictReg[\"Y\"]\n",
    "#\n",
    "yPredict = dictReg[\"yPredict\"]\n",
    "#\n",
    "# all points (with little noise)\n",
    "plt.scatter(xPoints, yPoints, color='gray', label=\"datapoints (noisy)\")\n",
    "# regression line from regression model\n",
    "plt.plot(xPoints, yPredict, color='red', linewidth=2, label=\"regression line\")\n",
    "# linear function of clean data (y = m*x + b)\n",
    "plt.plot(clearLinData[:,0], clearLinData[:,1], color='orange', lw=2, label=\"linear function: y={}*x+{}\".format(slope, intercept))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_2'></a><br>**Example 2**\n",
    "\n",
    "more datapoints and stronger distotion:\n",
    "\n",
    "Create dataset using function [get_DataPoints()](#get_DataPoints) und [create_noiseVec()](#create_noiseVec). The predefined parameters allow following linear function:<br>\n",
    "y = 0.45 * x + 0.5 \n",
    "total number of points: 200\n",
    "x-range: 0...10\n",
    "<br>\n",
    "distortion: 4.0\n",
    "<br>\n",
    "After regression is computed with [calc_regression](#calc_regression) the predicted values are plotted together with the noisy datapoints and the 'true' linear function.<br>\n",
    "more info about linear regression: [sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate a simple dataset with strong distortion\n",
    "#\n",
    "#\n",
    "xMin = 0\n",
    "xMax = 10\n",
    "#\n",
    "nPoints = 200 # total number of points \n",
    "#\n",
    "slope = 0.45 #(m)\n",
    "#\n",
    "intercept = 0.5 #(b)\n",
    "#\n",
    "clearLinData = get_DataPoints(nPoints, slope, intercept, xMax, xMin)\n",
    "#\n",
    "# create some noise\n",
    "#\n",
    "noiseData = create_noiseVec(4.0, nPoints)\n",
    "#\n",
    "# add noise to the linear points\n",
    "noisyPoints = clearLinData + noiseData\n",
    "#\n",
    "# calc linear regression\n",
    "#\n",
    "dictReg = calc_regression(noisyPoints)\n",
    "#\n",
    "# test regression model\n",
    "\n",
    "# plot data\n",
    "#\n",
    "# \n",
    "xPoints = dictReg[\"X\"]\n",
    "yPoints = dictReg[\"Y\"]\n",
    "#\n",
    "yPredict = dictReg[\"yPredict\"]\n",
    "#\n",
    "# all points (with little noise)\n",
    "plt.scatter(xPoints, yPoints, color='gray', label=\"datapoints (noisy)\")\n",
    "# regression line from regression model\n",
    "plt.plot(xPoints, yPredict, color='red', linewidth=2, label=\"regression line\")\n",
    "# linear function of clean data (y = m*x + b)\n",
    "plt.plot(clearLinData[:,0], clearLinData[:,1], color='orange', lw=2, label=\"linear function: y={}*x+{}\".format(slope, intercept))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_3'></a><br>**Example 3**\n",
    "\n",
    "#### Linear Regression with outliers\n",
    "\n",
    "\n",
    "Create dataset using function [get_DataPoints()](#get_DataPoints) und [create_noiseVec()](#create_noiseVec). The predefined parameters allow following linear function:<br>\n",
    "- y = 0.45 * x + 0.5 \n",
    "- nPoints:  100 (total number of points)\n",
    "- xMin, xMax: 0...5 (x-range)\n",
    "- nOutliers: 5 (number of outlier points) \n",
    "<br>\n",
    "- distortion: 0.6\n",
    "\n",
    "After regression is computed with [calc_regression](#calc_regression) the predicted values are plotted together with the noisy datapoints and the 'true' linear function.<br>\n",
    "more info about linear regression: [sckit-learn.LinearModels](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate a simple dataset with some noise and outlier points\n",
    "#\n",
    "np.random.seed(0)\n",
    "#\n",
    "xMin = 0\n",
    "xMax = 5\n",
    "#\n",
    "nPoints = 1000  \n",
    "#\n",
    "slope = 0.45\n",
    "#\n",
    "intercept = 0.5\n",
    "#\n",
    "nOutliers = 5\n",
    "#\n",
    "distortion = 0.6\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "# define clear dataset\n",
    "clearLinData = get_DataPoints((nPoints + nOutliers), slope, intercept, xMax, xMin)\n",
    "#\n",
    "# add Outliers\n",
    "clearLinData[nPoints:,0] = 15 + 0.5 * np.random.normal(size=(nOutliers))\n",
    "clearLinData[nPoints:,1] = -3 - 10 * np.random.normal(size=nOutliers)\n",
    "#\n",
    "# create some noise\n",
    "noiseData = create_noiseVec(distortion, (nPoints + nOutliers))\n",
    "# add noise to the linear points\n",
    "noisyPoints = clearLinData + noiseData\n",
    "#\n",
    "# calc linear regression\n",
    "#\n",
    "dictReg = calc_regression(noisyPoints)\n",
    "#\n",
    "######################################\n",
    "# plot data\n",
    "#\n",
    "# \n",
    "xPoints = dictReg[\"X\"]\n",
    "yPoints = dictReg[\"Y\"]\n",
    "#\n",
    "yPredict = dictReg[\"yPredict\"]\n",
    "#\n",
    "# all points (with little noise)\n",
    "plt.scatter(xPoints, yPoints, color='gray', label=\"datapoints (noisy)\")\n",
    "# regression line from regression model\n",
    "plt.plot(xPoints, yPredict, color='red', linewidth=2, label=\"regression line\")\n",
    "# linear function of clean data (y = m*x + b)\n",
    "plt.plot(clearLinData[0:nPoints,0], clearLinData[0:nPoints,1], color='orange', lw=2, label=\"linear function: y={}*x+{}\".format(slope, intercept))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularized_regression'></a><br>\n",
    "#### Regularized Regression\n",
    "\n",
    "\"Regularization is a very important concept that is used to avoid overfitting of the data especially when the trained and tested data are much varying.\n",
    "Regularization is implemented by adding a “penalty” term to the best fit derived from the trained data, in order to achieve a lesser variance with the tested data and also restricts the influence of predictor variables over the output variable by compressing their coefficients.\" [1]\n",
    "In the following examples 2 methods are shown:\n",
    "- ridge regression\n",
    "- Lasso\n",
    "\n",
    "##### Ridge regression\n",
    "\"Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:\n",
    "$$ \\min\\limits_{\\omega} || X\\omega-y||_2^2 + \\alpha ||\\omega||_2^2$$\n",
    "The complexity parameter $\\alpha \\geq 0$ controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n",
    "As with other linear models, Ridge will take in its ${fit}$ method arrays $X, y$ and will store the coefficients  of the linear model in its $\\text{coef_}$ member.\"[2]<br>\n",
    "more info about ridge regression: [scikit-learn.ridge-regession:](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)<br>\n",
    "\n",
    "##### Lasso\n",
    "\"The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients (see [Compressive sensing: tomography reconstruction with L1 prior (Lasso)](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py)).\n",
    "Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:\n",
    "$$ \\min\\limits_{\\omega} \\frac{1}{2n_{samples}} || X\\omega-y||_2^2 + \\alpha ||\\omega||_1$$\n",
    " \n",
    "The lasso estimate thus solves the minimization of the least-squares penalty with $\\alpha||\\omega||_1$ added, where $\\alpha$ is a constant and $||\\omega||_1$ is the $\\ell_1$-norm of the coefficient vector.\n",
    "The implementation in the class Lasso uses coordinate descent as the algorithm to fit the coefficients. See [Least Angle Regression](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) for another implementation\" [3]\n",
    "\n",
    "more info about Lasso: [scikit-learn.ridge-regession:](https://scikit-learn.org/stable/modules/linear_model.html#lasso)<br>\n",
    "\n",
    "<a id='1'></a> [[1]](https://towardsdatascience.com/regression-with-regularization-techniques-7bbc1a26d9ba)<br>\n",
    "<a id='2'></a> [[2]](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)<br>\n",
    "<a id='3'></a> [[3]](https://scikit-learn.org/stable/modules/linear_model.html#lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example_4\"></a><br>\n",
    "**example 4**\n",
    "regularized regression with ridge regression and lasso\n",
    "\n",
    "In this example the slope and intercept value from the ridge and lasso regression will be displayd in a histogram plot. To demonstrate the 2 methods, 3 datasets with different slopes will be created and fitted to the both models. This process will be repeated for 1000 times.\n",
    "\n",
    "functions:\n",
    "- f1: $y=0.0\\bullet x+ 1.0$\n",
    "- f2: $y=0.2\\bullet x+ 1.0$\n",
    "- f3: $y=2.0\\bullet x+ 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "alpha = 1\n",
    "#\n",
    "xMin = 0\n",
    "#\n",
    "xMax = 5\n",
    "#\n",
    "nPoints = 500\n",
    "#\n",
    "distortion= 0.5\n",
    "#\n",
    "nRepeats = 1000\n",
    "#\n",
    "slopes=[0.0, 0.2, 2.0]\n",
    "#\n",
    "intercept = [1.0, 1.0, 1.0]\n",
    "#\n",
    "coef_result = np.empty([nRepeats,6])\n",
    "intercpt_result = np.empty([nRepeats,6])\n",
    "#\n",
    "ridgeModel = Ridge(alpha=alpha)\n",
    "lassoModel = Lasso(alpha=alpha, max_iter=1e5)\n",
    "#\n",
    "for i in range(nRepeats):\n",
    "    #\n",
    "    noise = create_noiseVec(distortion, nPoints)\n",
    "    #\n",
    "    set1 = get_DataPoints(nPoints, slopes[0], intercept[0], xMax, xMin) + noise\n",
    "    set2 = get_DataPoints(nPoints, slopes[1], intercept[1], xMax, xMin) + noise\n",
    "    set3 = get_DataPoints(nPoints, slopes[2], intercept[2], xMax, xMin) + noise\n",
    "    #\n",
    "    # fit and analyse set1\n",
    "    ridgeModel.fit(set1[:,0].reshape(-1,1), set1[:,1].reshape(-1,1))\n",
    "    coef_result[i,0] = ridgeModel.coef_\n",
    "    intercpt_result[i,0] = ridgeModel.intercept_\n",
    "    #\n",
    "    lassoModel.fit(set1[:,0].reshape(-1,1), set1[:,1].reshape(-1,1))\n",
    "    coef_result[i,3] = lassoModel.coef_\n",
    "    intercpt_result[i,3] = lassoModel.intercept_\n",
    "    #\n",
    "    # fit and analyse set2\n",
    "    ridgeModel.fit(set2[:,0].reshape(-1,1), set2[:,1].reshape(-1,1))\n",
    "    coef_result[i,1] = ridgeModel.coef_\n",
    "    intercpt_result[i,1] = ridgeModel.intercept_\n",
    "    #\n",
    "    lassoModel.fit(set2[:,0].reshape(-1,1), set2[:,1].reshape(-1,1))\n",
    "    coef_result[i,4] = lassoModel.coef_\n",
    "    intercpt_result[i,4] = lassoModel.intercept_\n",
    "    #\n",
    "    # fit and analyse set3\n",
    "    ridgeModel.fit(set3[:,0].reshape(-1,1), set3[:,1].reshape(-1,1))\n",
    "    coef_result[i,2] = ridgeModel.coef_\n",
    "    intercpt_result[i,2] = ridgeModel.intercept_\n",
    "    #\n",
    "    lassoModel.fit(set3[:,0].reshape(-1,1), set3[:,1].reshape(-1,1))\n",
    "    coef_result[i,5] = lassoModel.coef_\n",
    "    intercpt_result[i,5] = lassoModel.intercept_\n",
    "    #\n",
    "#----------------------------------------------------------------\n",
    "#\n",
    "# plot results\n",
    "#\n",
    "# slopes:                        interceptions:\n",
    "# |--0..2--| ... |--3...5--|     |--0..2--| ... |--3...5--|\n",
    "# |ridge() | ... | lasso()|      |ridge() | ... | lasso()|\n",
    "# | set1-3 | ... | set1-3 |      | set1-3 | ... | set1-3 |\n",
    "#\n",
    "#\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15,4), sharey=True)\n",
    "axes[0].hist(coef_result[:,0:3], 55)\n",
    "axes[1].hist(intercpt_result[:,0:3], 13)\n",
    "axes[2].hist(coef_result[:,4:6], 55)\n",
    "axes[3].hist(intercpt_result[:,4:6], 13)\n",
    "#\n",
    "axes[0].set_xticks(slopes)\n",
    "# axes[0].set_ylim([0,1000])\n",
    "axes[1].set_xticks(intercept)\n",
    "axes[2].set_xticks(slopes)\n",
    "axes[3].set_xticks(intercept)\n",
    "\n",
    "#\n",
    "str0 = r\"ridge-regression with $\\alpha = {{{}}}$\".format(alpha)\n",
    "str0 += \"\\n\"\n",
    "str0 += r\"y = {} $\\bullet$ x + {}\".format(slopes[0], intercept[0])\n",
    "str0 += \"\\n\"\n",
    "str0 += r\"y = {} $\\bullet$ x + {}\".format(slopes[1], intercept[1])\n",
    "str0 += \"\\n\"\n",
    "str0 += r\"y = {} $\\bullet$ x + {}\".format(slopes[2], intercept[2])\n",
    "axes[0].set_title(str0)\n",
    "axes[0].legend(['slope: %.1f'%slopes[0], 'slope: %.1f'%slopes[1], 'slope: %.1f'%slopes[2]])\n",
    "axes[1].set_title(str0)\n",
    "axes[1].legend(['intercept: %.1f' %intercept[0], 'intercept: %.1f' %intercept[1], 'intercept: %.1f' %intercept[2]])\n",
    "#\n",
    "str1 = str0.replace('ridge', 'lasso')\n",
    "axes[2].set_title(str1)\n",
    "axes[2].legend(['slope: %.1f'%slopes[0], 'slope: %.1f'%slopes[1], 'slope: %.1f'%slopes[2]])\n",
    "axes[3].set_title(str1)\n",
    "axes[3].legend(['intercept: %.1f' %intercept[0], 'intercept: %.1f' %intercept[1], 'intercept: %.1f' %intercept[2]])\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example_5\"></a>**Example 5**\n",
    "\n",
    "Regularized Regression with polynomial dataset\n",
    "\n",
    "A polynomial function will be represented with just a litte amount of data points. This training data will be used for different regression models. The result can be seen in the following diagrams:\n",
    "\n",
    "polynomial function:\n",
    "$y= -10 \\bullet x^3 - 2.3 \\bullet x^2 + 0.1\\bullet x -2$\n",
    "\n",
    "number of training points (nPoints): 40\n",
    "xMin: -2\n",
    "xMax: 2\n",
    "distortion: 0.5\n",
    "alpha ridge: 0.5\n",
    "alpha lasso: 0.5\n",
    "degree polyfit: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso \n",
    "#\n",
    "# sample function: y = -10x³ - 2.3x² + 0.1x -2\n",
    "#\n",
    "def f(x):\n",
    "    return(-10*x**3 - 2.3*x**2 +0.1*x -2)\n",
    "\n",
    "#\n",
    "# create dataset\n",
    "# generate points and keep a subset of them\n",
    "xMin=-2\n",
    "xMax=2\n",
    "#\n",
    "nPoints = 20\n",
    "#\n",
    "alpha_ridge = 2.0\n",
    "alpha_lasso = 3.0\n",
    "#\n",
    "#\n",
    "x_0 = np.linspace(xMin,xMax,200)\n",
    "x_plot = np.linspace(xMin,xMax,200)\n",
    "#\n",
    "#\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x_0)\n",
    "x = np.sort(x_0[:nPoints])\n",
    "y = f(x)\n",
    "#\n",
    "X = x[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "#--------------------------------------------------\n",
    "#\n",
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "# linear regression\n",
    "#\n",
    "polynomial_features = PolynomialFeatures(degree=5)\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", LinearRegression())])\n",
    "# linModel = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
    "pipeline.fit(X,y)\n",
    "LinPlot = pipeline.predict(X_plot)\n",
    "plt.plot(x_plot, LinPlot, color=colors[0], linewidth=lw, label=\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "# poly fitting\n",
    "polyFit = np.poly1d(np.polyfit(x,y,3))\n",
    "plt.plot(x_plot, polyFit(x_plot), color=colors[1], lw=lw, label=\"polyfit\")\n",
    "#\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "#LR + ridge\n",
    "LR_ridge = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=3)),\n",
    "                         (\"linear_regression\", Ridge(alpha=2))\n",
    "                    ])\n",
    "LR_ridge.fit(X,y)\n",
    "yLR_ridge = LR_ridge.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_ridge, color=colors[2],lw=lw,label=\"LR + ridge\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show training data with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "\n",
    "#LR + lasso\n",
    "LR_lasso = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=3)),\n",
    "                         (\"linear_regression\", Lasso(alpha = 5))\n",
    "                    ])\n",
    "LR_lasso.fit(X,y)\n",
    "yLR_lasso = LR_lasso.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_lasso, color=colors[3],lw=lw,label=\"LR + Lasso\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot all regression results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "#\n",
    "colors = ['teal', 'yellowgreen', 'gold', 'gray']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"truth\")\n",
    "#\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "#\n",
    "# linear regression\n",
    "#\n",
    "polynomial_features = PolynomialFeatures(degree=1)\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", LinearRegression())])\n",
    "# linModel = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
    "pipeline.fit(X,y)\n",
    "LinPlot = pipeline.predict(X_plot)\n",
    "plt.plot(x_plot, LinPlot, color=colors[0], linewidth=lw, label=\"Linear Regression\")\n",
    "#\n",
    "# poly fitting\n",
    "polyFit = np.poly1d(np.polyfit(x,y,3))\n",
    "plt.plot(x_plot, polyFit(x_plot), color=colors[1], lw=lw, label=\"polyfit\")\n",
    "#\n",
    "#LR + ridge\n",
    "LR_ridge = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=4)),\n",
    "                         (\"linear_regression\", Ridge(alpha=alpha_ridge))\n",
    "                    ])\n",
    "LR_ridge.fit(X,y)\n",
    "yLR_ridge = LR_ridge.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_ridge, color=colors[2],lw=lw,label=\"LR + ridge\")\n",
    "#\n",
    "#LR + lasso\n",
    "LR_lasso = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=6)),\n",
    "                         (\"linear_regression\", Lasso(alpha=alpha_lasso))])\n",
    "LR_lasso.fit(X,y)\n",
    "yLR_lasso = LR_lasso.predict(X_plot)\n",
    "#\n",
    "plt.plot(X_plot, yLR_lasso, color=colors[3],lw=lw,label=\"LR + Lasso\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 IUBH Internationale Hochschule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
